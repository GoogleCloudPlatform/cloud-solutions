// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import {Conversation, ConversationEntry} from '@/types/chat';
import ChatService from './chatService';

export class FollowUpAnalysisService {
  // Generate AI response to follow-up questions based on conversation context
  static async generateFollowUpResponse(
    userMessage: string,
    conversationId: string
  ): Promise<string> {
    const conversation = ChatService.getConversation(conversationId);
    if (!conversation) {
      return this.getGenericResponse(userMessage);
    }

    // Get the most recent widget analysis from the conversation
    const widgetAnalysis = this.getLatestWidgetAnalysis(conversation);
    if (!widgetAnalysis) {
      return this.getGenericResponse(userMessage);
    }

    // Generate contextual response based on the original widget analysis
    return this.generateContextualResponse(userMessage, widgetAnalysis);
  }

  private static getLatestWidgetAnalysis(
    conversation: Conversation
  ): ConversationEntry | null {
    // Find the most recent assistant message with widget metadata
    for (let i = conversation.entries.length - 1; i >= 0; i--) {
      const entry = conversation.entries[i];
      if (entry.type === 'assistant' && entry.metadata?.widgetId) {
        return entry;
      }
    }
    return null;
  }

  private static generateContextualResponse(
    userMessage: string,
    originalAnalysis: ConversationEntry
    // conversation: Conversation // This parameter was unused
  ): string {
    const widgetId = originalAnalysis.metadata?.widgetId;

    // Normalize user message for keyword matching
    const message = userMessage.toLowerCase();

    // Revenue/Response Time follow-ups
    if (widgetId?.includes('revenue') || widgetId?.includes('response-time')) {
      return this.generateRevenueFollowUp(
        message, // userMessage normalized
        originalAnalysis.metadata
      );
    }

    // System Health follow-ups
    if (widgetId?.includes('system-health') || widgetId?.includes('service')) {
      return this.generateSystemHealthFollowUp(
        message, // userMessage normalized
        originalAnalysis.metadata
      );
    }

    // Error Rate follow-ups
    if (widgetId?.includes('error-rate') || widgetId?.includes('error')) {
      return this.generateErrorRateFollowUp(
        message, // userMessage normalized
        originalAnalysis.metadata
      );
    }

    // General follow-up
    return this.generateGeneralFollowUp(message, originalAnalysis);
  }

  private static generateRevenueFollowUp(
    message: string,
    metrics: ConversationEntry['metadata']
  ): string {
    if (
      message.includes('historical') ||
      message.includes('trend') ||
      message.includes('history')
    ) {
      return `ðŸ“Š **Historical Response Time Analysis**

Based on the last 7 days of data, here are the key trends:

**Response Time Patterns:**
â€¢ Average: 180ms (within optimal range)
â€¢ Peak hours (2-4 PM): 280ms average
â€¢ Off-peak hours: 120ms average
â€¢ 95th percentile: ${metrics?.responseTime || 250}ms

**Revenue Correlation:**
â€¢ Strong negative correlation (-0.78) between response time and conversion
â€¢ Every 100ms increase = ~3.2% revenue decrease
â€¢ Optimal performance window: <200ms response time

**Recommendations:**
â€¢ Implement auto-scaling during peak hours
â€¢ Consider CDN optimization for static assets
â€¢ Monitor database query performance during high traffic`;
    }

    if (message.includes('payment') || message.includes('gateway')) {
      return `ðŸ’³ **Payment Gateway Analysis**

Current payment processing performance:

**Gateway Metrics:**
â€¢ Payment success rate: 98.7%
â€¢ Average processing time: 1.8s
â€¢ Failed payments: 1.3% (mostly timeout-related)
â€¢ Peak processing time: ${metrics?.responseTime || 250}ms additional latency

**Impact Assessment:**
â€¢ Payment timeouts correlate with high response times
â€¢ Revenue impact: ~$${Math.round((metrics?.responseTime || 250) * 0.5)}/hour during spikes

**Immediate Actions:**
â€¢ Check payment gateway connection pool
â€¢ Implement payment retry logic
â€¢ Monitor third-party payment API status
â€¢ Consider backup payment processor`;
    }

    if (
      message.includes('alert') ||
      message.includes('monitor') ||
      message.includes('notification')
    ) {
      return `ðŸ”” **Alert Configuration Recommendations**

I recommend setting up these monitoring alerts:

**Response Time Alerts:**
â€¢ Warning: >250ms for 2 consecutive minutes
â€¢ Critical: >400ms for 1 minute
â€¢ Recovery: <200ms for 5 minutes

**Revenue Impact Alerts:**
â€¢ Revenue drop >$100/hour compared to baseline
â€¢ Conversion rate decrease >5%
â€¢ Payment failure rate >2%

**Suggested Alert Channels:**
â€¢ Slack #ops-alerts for warnings
â€¢ PagerDuty for critical issues
â€¢ Email dashboard for daily summaries

Would you like me to help configure any specific alert thresholds?`;
    }

    return `Based on the revenue vs response time analysis, I can provide more details about:
â€¢ Historical performance trends over the last 30 days
â€¢ Payment gateway-specific metrics and optimizations
â€¢ Alert configuration for proactive monitoring
â€¢ Database query optimization recommendations

What specific aspect would you like me to elaborate on?`;
  }

  private static generateSystemHealthFollowUp(
    message: string,
    metrics: ConversationEntry['metadata']
  ): string {
    const serviceName = metrics?.serviceName || 'the service';

    if (
      message.includes('dependency') ||
      message.includes('dependencies') ||
      message.includes('map')
    ) {
      return `ðŸ”— **Service Dependency Map for ${serviceName}**

**Upstream Dependencies:**
â€¢ Database (PostgreSQL): ${(metrics?.healthScore || 0) > 80 ? 'Healthy' : 'Degraded'}
â€¢ Redis Cache: Healthy
â€¢ Payment Gateway API: Healthy
â€¢ Authentication Service: Healthy

**Downstream Services:**
â€¢ Frontend Application: 3 instances
â€¢ Mobile API Gateway: 2 instances
â€¢ Analytics Pipeline: 1 instance

**Critical Path Impact:**
â€¢ ${serviceName} failure affects 4 downstream services
â€¢ Estimated impact: ${(metrics?.healthScore || 0) < 50 ? 'High' : (metrics?.healthScore || 0) < 80 ? 'Medium' : 'Low'} user impact
â€¢ Recovery time: 2-5 minutes with auto-scaling

**Recommended Actions:**
â€¢ Implement circuit breaker patterns
â€¢ Add health check endpoints
â€¢ Configure graceful degradation`;
    }

    if (
      message.includes('scale') ||
      message.includes('scaling') ||
      message.includes('resource')
    ) {
      return `ðŸ“ˆ **Scaling Recommendations for ${serviceName}**

**Current Resource Utilization:**
â€¢ CPU: ${metrics?.cpu || 0}%
â€¢ Memory: ${metrics?.memory || 0}%
â€¢ Response Time: ${metrics?.responseTime || 0}ms

**Scaling Analysis:**
${
  (metrics?.cpu || 0) > 80
    ? 'â€¢ **Immediate scaling needed**: CPU usage critical'
    : (metrics?.cpu || 0) > 60
      ? 'â€¢ **Proactive scaling**: CPU approaching limits'
      : 'â€¢ **Resources adequate**: No immediate scaling needed'
}

**Recommendations:**
â€¢ Horizontal scaling: Add 1-2 instances during peak hours
â€¢ Vertical scaling: Consider upgrading to next tier if sustained high usage
â€¢ Auto-scaling triggers: CPU >70% or Response Time >300ms

**Cost Impact:**
â€¢ Additional instance cost: ~$50/month
â€¢ Potential revenue protection: $500-2000/month`;
    }

    if (
      message.includes('log') ||
      message.includes('debug') ||
      message.includes('troubleshoot')
    ) {
      return `ðŸ” **Troubleshooting Guide for ${serviceName}**

**Log Analysis Steps:**
1. Check application logs: \`kubectl logs -f ${serviceName}-pod\`
2. Review error patterns in the last hour
3. Monitor resource metrics in real-time
4. Check database connection status

**Common Issues & Solutions:**
â€¢ **Memory leaks**: Look for gradual memory increase over time
â€¢ **Database timeouts**: Check connection pool settings
â€¢ **API rate limits**: Review third-party service quotas
â€¢ **Network issues**: Verify service mesh connectivity

**Debug Commands:**
â€¢ Health check: \`curl ${serviceName}/health\`
â€¢ Metrics endpoint: \`curl ${serviceName}/metrics\`
â€¢ Memory profile: Available in admin dashboard

Would you like me to walk through any specific troubleshooting steps?`;
    }

    return `For ${serviceName} health analysis, I can provide more information about:
â€¢ Service dependency mapping and impact analysis
â€¢ Resource scaling recommendations and cost analysis
â€¢ Detailed troubleshooting steps and log analysis
â€¢ Historical performance comparisons

What specific aspect would you like me to explore further?`;
  }

  private static generateErrorRateFollowUp(
    message: string,
    metrics: ConversationEntry['metadata']
  ): string {
    if (
      message.includes('type') ||
      message.includes('breakdown') ||
      message.includes('category')
    ) {
      return `ðŸ“Š **Error Breakdown Analysis**

**Error Distribution (Last 24 Hours):**
â€¢ 4xx Client Errors: 45% of total errors
  - 404 Not Found: 32%
  - 400 Bad Request: 8%
  - 401 Unauthorized: 5%

â€¢ 5xx Server Errors: 55% of total errors
  - 500 Internal Server Error: 28%
  - 503 Service Unavailable: 15%
  - 502 Bad Gateway: 12%

**Error Rate Context:**
â€¢ Current rate: ${metrics?.errorRate || 0}%
â€¢ Baseline: 0.5%
â€¢ Peak today: ${Math.max(5, (metrics?.errorRate || 0) * 1.5)}%

**Top Error Sources:**
â€¢ Payment processing endpoint: 35%
â€¢ User authentication: 25%
â€¢ Database queries: 20%
â€¢ External API calls: 20%`;
    }

    if (
      message.includes('pattern') ||
      message.includes('time') ||
      message.includes('when')
    ) {
      return `â° **Error Pattern Analysis**

**Temporal Patterns:**
â€¢ Peak error times: 2-4 PM, 8-10 PM (traffic spikes)
â€¢ Lowest errors: 3-6 AM
â€¢ Weekend vs Weekday: 23% higher errors on weekends

**Error Frequency Patterns:**
â€¢ Batch processing errors: Every 15 minutes
â€¢ Authentication errors: Correlate with login attempts
â€¢ Payment errors: Higher during sale events

**Correlation Analysis:**
â€¢ Traffic vs Errors: Strong correlation (r=0.72)
â€¢ Response Time vs Errors: Moderate correlation (r=0.45)
â€¢ Memory Usage vs Errors: Weak correlation (r=0.23)

**Predicted Error Windows:**
â€¢ Next 2 hours: ${(metrics?.errorRate || 0) > 3 ? 'High risk' : 'Normal risk'}
â€¢ Peak traffic (2-4 PM): Elevated risk
â€¢ Maintenance window (2 AM): Very low risk`;
    }

    if (
      message.includes('custom') ||
      message.includes('alert') ||
      message.includes('threshold')
    ) {
      return `ðŸš¨ **Custom Alert Configuration**

**Recommended Alert Thresholds:**
â€¢ **Warning**: Error rate >2% for 5 minutes
â€¢ **Critical**: Error rate >5% for 2 minutes
â€¢ **Severe**: Error rate >10% for 1 minute

**Smart Alert Rules:**
â€¢ Suppress alerts during known maintenance windows
â€¢ Escalate if error rate increases 300% from baseline
â€¢ Auto-resolve when error rate <1% for 10 minutes

**Alert Channels by Severity:**
â€¢ Warning: Slack #engineering-alerts
â€¢ Critical: PagerDuty + Slack #incident-response
â€¢ Severe: PagerDuty + SMS + Phone call

**Custom Filters:**
â€¢ Exclude 404 errors from static assets
â€¢ Include only business-critical endpoint errors
â€¢ Group related errors to reduce alert fatigue

Would you like me to configure any of these alert rules?`;
    }

    return `Based on the error rate analysis, I can provide more details about:
â€¢ Error type breakdown and categorization
â€¢ Temporal patterns and correlation analysis
â€¢ Custom alert thresholds and notification setup
â€¢ Root cause analysis for specific error types

What specific aspect would you like me to investigate further?`;
  }

  private static generateGeneralFollowUp(
    message: string,
    originalAnalysis: ConversationEntry
  ): string {
    if (
      message.includes('what') &&
      (message.includes('do') || message.includes('next'))
    ) {
      return `Based on our analysis, here are the recommended next steps:

${
  originalAnalysis.metadata?.actionSuggestions
    ?.map((action: string, i: number) => `${i + 1}. ${action}`)
    .join('\n') ||
  'â€¢ Review the metrics and trends\nâ€¢ Set up monitoring for similar issues'
}

**Priority Actions:**
â€¢ Immediate: Address any critical issues identified
â€¢ Short-term: Implement monitoring and alerts
â€¢ Long-term: Optimize performance based on trends

Would you like me to elaborate on any specific action item?`;
    }

    if (
      message.includes('explain') ||
      message.includes('why') ||
      message.includes('how')
    ) {
      return `Let me explain the technical details behind this analysis:

The analysis is based on correlating multiple data points including response times, error rates, resource utilization, and business metrics. Here's how the system works:

â€¢ **Data Collection**: Real-time metrics from multiple sources
â€¢ **Pattern Recognition**: Statistical analysis to identify anomalies
â€¢ **Impact Assessment**: Business logic to calculate potential impact
â€¢ **Recommendations**: Best practices matched to your specific scenario

The insights are generated using machine learning models trained on historical patterns and industry benchmarks.

Is there a specific technical aspect you'd like me to dive deeper into?`;
    }

    return `I can provide more specific guidance based on our previous analysis. Could you clarify what aspect you'd like me to elaborate on? For example:

â€¢ Technical implementation details
â€¢ Specific troubleshooting steps
â€¢ Historical data comparisons
â€¢ Business impact calculations

Feel free to ask about any part of the analysis!`;
  }

  private static getGenericResponse(message: string): string {
    const responses = [
      "I'd be happy to help! Could you provide more context about what you're looking for?",
      "Let me assist you with that. Can you clarify which aspect of the dashboard you're interested in?",
      "I'm here to help analyze your data. What specific insights are you looking for?",
      "Could you be more specific about what you'd like me to analyze or explain?",
      'I can provide detailed analysis on any of the dashboard widgets. What would you like to explore?',
    ];
    // Reserve `message` for future releases.
    console.log(`getGenericResponse(message):${message}`);
    return responses[Math.floor(Math.random() * responses.length)];
  }
}

export default FollowUpAnalysisService;
