# ADK Python Repository



# Agent Development Kit (ADK)

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Python Unit Tests](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg)](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml)
[![r/agentdevelopmentkit](https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&logo=reddit&logoColor=white)](https://www.reddit.com/r/agentdevelopmentkit/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/google/adk-python)

<html>
    <h2 align="center">
      <img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" width="256"/>
    </h2>
    <h3 align="center">
      An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.
    </h3>
    <h3 align="center">
      Important Links:
      <a href="https://google.github.io/adk-docs/">Docs</a>,
      <a href="https://github.com/google/adk-samples">Samples</a>,
      <a href="https://github.com/google/adk-java">Java ADK</a> &
      <a href="https://github.com/google/adk-web">ADK Web</a>.
    </h3>
</html>

Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and is built for compatibility with other frameworks. ADK was designed to make agent development feel more like software development, to make it easier for developers to create, deploy, and orchestrate agentic architectures that range from simple tasks to complex workflows.


---

## ‚ú® Key Features

- **Rich Tool Ecosystem**: Utilize pre-built tools, custom functions,
  OpenAPI specs, or integrate existing tools to give agents diverse
  capabilities, all for tight integration with the Google ecosystem.

- **Code-First Development**: Define agent logic, tools, and orchestration
  directly in Python for ultimate flexibility, testability, and versioning.

- **Modular Multi-Agent Systems**: Design scalable applications by composing
  multiple specialized agents into flexible hierarchies.

- **Deploy Anywhere**: Easily containerize and deploy agents on Cloud Run or
  scale seamlessly with Vertex AI Agent Engine.

## ü§ñ Agent2Agent (A2A) Protocol and ADK Integration

For remote agent-to-agent communication, ADK integrates with the
[A2A protocol](https://github.com/google-a2a/A2A/).
See this [example](https://github.com/google-a2a/a2a-samples/tree/main/samples/python/agents/google_adk)
for how they can work together.

## üöÄ Installation

### Stable Release (Recommended)

You can install the latest stable version of ADK using `pip`:

```bash
pip install google-adk
```

The release cadence is weekly.

This version is recommended for most users as it represents the most recent official release.

### Development Version
Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven't been included in an official PyPI release yet, you can install directly from the main branch:

```bash
pip install git+https://github.com/google/adk-python.git@main
```

Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.

## üìö Documentation

Explore the full documentation for detailed guides on building, evaluating, and
deploying agents:

* **[Documentation](https://google.github.io/adk-docs)**

### Agent Structure Convention (Required)

**All agent directories must follow this structure:**

```bash
my_agent/
‚îú‚îÄ‚îÄ __init__.py      # MUST contain: from . import agent
‚îî‚îÄ‚îÄ agent.py         # MUST define: root_agent = Agent(...) OR app = App(...)
```

## üèÅ Feature Highlight

### Define a single agent:

```python
from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name="search_assistant",
    model="gemini-2.5-flash", # Or your preferred Gemini model
    instruction="You are a helpful assistant. Answer user questions using Google Search when needed.",
    description="An assistant that can search the web.",
    tools=[google_search]
)
```

### Define a multi-agent system:

Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.

```python
from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name="greeter", model="gemini-2.5-flash", ...)
task_executor = LlmAgent(name="task_executor", model="gemini-2.5-flash", ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name="Coordinator",
    model="gemini-2.5-flash",
    description="I coordinate greetings and tasks.",
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
```

### Development UI

A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).

<img src="https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png"/>

###  Evaluate Agents

```bash
adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
```

## ü§ù Contributing

We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our
- [General contribution guideline and flow](https://google.github.io/adk-docs/contributing-guide/).
- Then if you want to contribute code, please read [Code Contributing Guidelines](./CONTRIBUTING.md) to get started.

## üìÑ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

---

*Happy Agent Building!*




---



!!! warning "Advanced Concept"

    Building custom agents by directly implementing `_run_async_impl` (or its equivalent in other languages) provides powerful control but is more complex than using the predefined `LlmAgent` or standard `WorkflowAgent` types. We recommend understanding those foundational agent types first before tackling custom orchestration logic.

# Custom agents

Custom agents provide the ultimate flexibility in ADK, allowing you to define **arbitrary orchestration logic** by inheriting directly from `BaseAgent` and implementing your own control flow. This goes beyond the predefined patterns of `SequentialAgent`, `LoopAgent`, and `ParallelAgent`, enabling you to build highly specific and complex agentic workflows.

## Introduction: Beyond Predefined Workflows

### What is a Custom Agent?

A Custom Agent is essentially any class you create that inherits from `google.adk.agents.BaseAgent` and implements its core execution logic within the `_run_async_impl` asynchronous method. You have complete control over how this method calls other agents (sub-agents), manages state, and handles events.

!!! Note
    The specific method name for implementing an agent's core asynchronous logic may vary slightly by SDK language (e.g., `runAsyncImpl` in Java, `_run_async_impl` in Python). Refer to the language-specific API documentation for details.

### Why Use Them?

While the standard [Workflow Agents](workflow-agents/index.md) (`SequentialAgent`, `LoopAgent`, `ParallelAgent`) cover common orchestration patterns, you'll need a Custom agent when your requirements include:

* **Conditional Logic:** Executing different sub-agents or taking different paths based on runtime conditions or the results of previous steps.
* **Complex State Management:** Implementing intricate logic for maintaining and updating state throughout the workflow beyond simple sequential passing.
* **External Integrations:** Incorporating calls to external APIs, databases, or custom libraries directly within the orchestration flow control.
* **Dynamic Agent Selection:** Choosing which sub-agent(s) to run next based on dynamic evaluation of the situation or input.
* **Unique Workflow Patterns:** Implementing orchestration logic that doesn't fit the standard sequential, parallel, or loop structures.


![intro_components.png](../assets/custom-agent-flow.png)


## Implementing Custom Logic:

The core of any custom agent is the method where you define its unique asynchronous behavior. This method allows you to orchestrate sub-agents and manage the flow of execution.

=== "Python"

      The heart of any custom agent is the `_run_async_impl` method. This is where you define its unique behavior.

      * **Signature:** `async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:`
      * **Asynchronous Generator:** It must be an `async def` function and return an `AsyncGenerator`. This allows it to `yield` events produced by sub-agents or its own logic back to the runner.
      * **`ctx` (InvocationContext):** Provides access to crucial runtime information, most importantly `ctx.session.state`, which is the primary way to share data between steps orchestrated by your custom agent.

=== "Java"

    The heart of any custom agent is the `runAsyncImpl` method, which you override from `BaseAgent`.

    *   **Signature:** `protected Flowable<Event> runAsyncImpl(InvocationContext ctx)`
    *   **Reactive Stream (`Flowable`):** It must return an `io.reactivex.rxjava3.core.Flowable<Event>`. This `Flowable` represents a stream of events that will be produced by the custom agent's logic, often by combining or transforming multiple `Flowable` from sub-agents.
    *   **`ctx` (InvocationContext):** Provides access to crucial runtime information, most importantly `ctx.session().state()`, which is a `java.util.concurrent.ConcurrentMap<String, Object>`. This is the primary way to share data between steps orchestrated by your custom agent.

**Key Capabilities within the Core Asynchronous Method:**

=== "Python"

    1. **Calling Sub-Agents:** You invoke sub-agents (which are typically stored as instance attributes like `self.my_llm_agent`) using their `run_async` method and yield their events:

          ```python
          async for event in self.some_sub_agent.run_async(ctx):
              # Optionally inspect or log the event
              yield event # Pass the event up
          ```

    2. **Managing State:** Read from and write to the session state dictionary (`ctx.session.state`) to pass data between sub-agent calls or make decisions:
          ```python
          # Read data set by a previous agent
          previous_result = ctx.session.state.get("some_key")

          # Make a decision based on state
          if previous_result == "some_value":
              # ... call a specific sub-agent ...
          else:
              # ... call another sub-agent ...

          # Store a result for a later step (often done via a sub-agent's output_key)
          # ctx.session.state["my_custom_result"] = "calculated_value"
          ```

    3. **Implementing Control Flow:** Use standard Python constructs (`if`/`elif`/`else`, `for`/`while` loops, `try`/`except`) to create sophisticated, conditional, or iterative workflows involving your sub-agents.

=== "Java"

    1. **Calling Sub-Agents:** You invoke sub-agents (which are typically stored as instance attributes or objects) using their asynchronous run method and return their event streams:

           You typically chain `Flowable`s from sub-agents using RxJava operators like `concatWith`, `flatMapPublisher`, or `concatArray`.


           The `Flowable.defer()` is often used for subsequent stages if their execution depends on the completion or state after prior stages.

    2. **Managing State:** Read from and write to the session state to pass data between sub-agent calls or make decisions. The session state is a `java.util.concurrent.ConcurrentMap<String, Object>` obtained via `ctx.session().state()`.



    3. **Implementing Control Flow:** Use standard language constructs (`if`/`else`, loops, `try`/`catch`) combined with reactive operators (RxJava) to create sophisticated workflows.

          *   **Conditional:** `Flowable.defer()` to choose which `Flowable` to subscribe to based on a condition, or `filter()` if you're filtering events within a stream.
          *   **Iterative:** Operators like `repeat()`, `retry()`, or by structuring your `Flowable` chain to recursively call parts of itself based on conditions (often managed with `flatMapPublisher` or `concatMap`).

## Managing Sub-Agents and State

Typically, a custom agent orchestrates other agents (like `LlmAgent`, `LoopAgent`, etc.).

* **Initialization:** You usually pass instances of these sub-agents into your custom agent's constructor and store them as instance fields/attributes (e.g., `this.story_generator = story_generator_instance` or `self.story_generator = story_generator_instance`). This makes them accessible within the custom agent's core asynchronous execution logic (such as: `_run_async_impl` method).
* **Sub Agents List:** When initializing the `BaseAgent` using it's `super()` constructor, you should pass a `sub agents` list. This list tells the ADK framework about the agents that are part of this custom agent's immediate hierarchy. It's important for framework features like lifecycle management, introspection, and potentially future routing capabilities, even if your core execution logic (`_run_async_impl`) calls the agents directly via `self.xxx_agent`. Include the agents that your custom logic directly invokes at the top level.
* **State:** As mentioned, `ctx.session.state` is the standard way sub-agents (especially `LlmAgent`s using `output key`) communicate results back to the orchestrator and how the orchestrator passes necessary inputs down.

## Design Pattern Example: `StoryFlowAgent`

Let's illustrate the power of custom agents with an example pattern: a multi-stage content generation workflow with conditional logic.

**Goal:** Create a system that generates a story, iteratively refines it through critique and revision, performs final checks, and crucially, *regenerates the story if the final tone check fails*.

**Why Custom?** The core requirement driving the need for a custom agent here is the **conditional regeneration based on the tone check**. Standard workflow agents don't have built-in conditional branching based on the outcome of a sub-agent's task. We need custom logic (`if tone == "negative": ...`) within the orchestrator.

---

### Part 1: Simplified custom agent Initialization

=== "Python"

    We define the `StoryFlowAgent` inheriting from `BaseAgent`. In `__init__`, we store the necessary sub-agents (passed in) as instance attributes and tell the `BaseAgent` framework about the top-level agents this custom agent will directly orchestrate.

    ```python
    class StoryFlowAgent(BaseAgent):
        """
        Custom agent for a story generation and refinement workflow.
        This agent orchestrates a sequence of LLM agents to generate a story,
        critique it, revise it, check grammar and tone, and potentially
        regenerate the story if the tone is negative.
        """
        # --- Field Declarations for Pydantic ---
        # Declare the agents passed during initialization as class attributes with type hints
        story_generator: LlmAgent
        critic: LlmAgent
        reviser: LlmAgent
        grammar_check: LlmAgent
        tone_check: LlmAgent
        loop_agent: LoopAgent
        sequential_agent: SequentialAgent
        # model_config allows setting Pydantic configurations if needed, e.g., arbitrary_types_allowed
        model_config = {"arbitrary_types_allowed": True}
        def __init__(
            self,
            name: str,
            story_generator: LlmAgent,
            critic: LlmAgent,
            reviser: LlmAgent,
            grammar_check: LlmAgent,
            tone_check: LlmAgent,
        ):
            """
            Initializes the StoryFlowAgent.
            Args:
                name: The name of the agent.
                story_generator: An LlmAgent to generate the initial story.
                critic: An LlmAgent to critique the story.
                reviser: An LlmAgent to revise the story based on criticism.
                grammar_check: An LlmAgent to check the grammar.
                tone_check: An LlmAgent to analyze the tone.
            """
            # Create internal agents *before* calling super().__init__
            loop_agent = LoopAgent(
                name="CriticReviserLoop", sub_agents=[critic, reviser], max_iterations=2
            )
            sequential_agent = SequentialAgent(
                name="PostProcessing", sub_agents=[grammar_check, tone_check]
            )
            # Define the sub_agents list for the framework
            sub_agents_list = [
                story_generator,
                loop_agent,
                sequential_agent,
            ]
            # Pydantic will validate and assign them based on the class annotations.
            super().__init__(
                name=name,
                story_generator=story_generator,
                critic=critic,
                reviser=reviser,
                grammar_check=grammar_check,
                tone_check=tone_check,
                loop_agent=loop_agent,
                sequential_agent=sequential_agent,
                sub_agents=sub_agents_list, # Pass the sub_agents list directly
            )
    ```

=== "Java"

    We define the `StoryFlowAgentExample` by extending `BaseAgent`. In its **constructor**, we store the necessary sub-agent instances (passed as parameters) as instance fields. These top-level sub-agents, which this custom agent will directly orchestrate, are also passed to the `super` constructor of `BaseAgent` as a list.


---

### Part 2: Defining the Custom Execution Logic

=== "Python"

    This method orchestrates the sub-agents using standard Python async/await and control flow.

    ```python
        @override
        async def _run_async_impl(
            self, ctx: InvocationContext
        ) -> AsyncGenerator[Event, None]:
            """
            Implements the custom orchestration logic for the story workflow.
            Uses the instance attributes assigned by Pydantic (e.g., self.story_generator).
            """
            logger.info(f"[{self.name}] Starting story generation workflow.")
            # 1. Initial Story Generation
            logger.info(f"[{self.name}] Running StoryGenerator...")
            async for event in self.story_generator.run_async(ctx):
                logger.info(f"[{self.name}] Event from StoryGenerator: {event.model_dump_json(indent=2, exclude_none=True)}")
                yield event
            # Check if story was generated before proceeding
            if "current_story" not in ctx.session.state or not ctx.session.state["current_story"]:
                 logger.error(f"[{self.name}] Failed to generate initial story. Aborting workflow.")
                 return # Stop processing if initial story failed
            logger.info(f"[{self.name}] Story state after generator: {ctx.session.state.get('current_story')}")
            # 2. Critic-Reviser Loop
            logger.info(f"[{self.name}] Running CriticReviserLoop...")
            # Use the loop_agent instance attribute assigned during init
            async for event in self.loop_agent.run_async(ctx):
                logger.info(f"[{self.name}] Event from CriticReviserLoop: {event.model_dump_json(indent=2, exclude_none=True)}")
                yield event
            logger.info(f"[{self.name}] Story state after loop: {ctx.session.state.get('current_story')}")
            # 3. Sequential Post-Processing (Grammar and Tone Check)
            logger.info(f"[{self.name}] Running PostProcessing...")
            # Use the sequential_agent instance attribute assigned during init
            async for event in self.sequential_agent.run_async(ctx):
                logger.info(f"[{self.name}] Event from PostProcessing: {event.model_dump_json(indent=2, exclude_none=True)}")
                yield event
            # 4. Tone-Based Conditional Logic
            tone_check_result = ctx.session.state.get("tone_check_result")
            logger.info(f"[{self.name}] Tone check result: {tone_check_result}")
            if tone_check_result == "negative":
                logger.info(f"[{self.name}] Tone is negative. Regenerating story...")
                async for event in self.story_generator.run_async(ctx):
                    logger.info(f"[{self.name}] Event from StoryGenerator (Regen): {event.model_dump_json(indent=2, exclude_none=True)}")
                    yield event
            else:
                logger.info(f"[{self.name}] Tone is not negative. Keeping current story.")
                pass
            logger.info(f"[{self.name}] Workflow finished.")
    ```
    **Explanation of Logic:**

    1. The initial `story_generator` runs. Its output is expected to be in `ctx.session.state["current_story"]`.
    2. The `loop_agent` runs, which internally calls the `critic` and `reviser` sequentially for `max_iterations` times. They read/write `current_story` and `criticism` from/to the state.
    3. The `sequential_agent` runs, calling `grammar_check` then `tone_check`, reading `current_story` and writing `grammar_suggestions` and `tone_check_result` to the state.
    4. **Custom Part:** The `if` statement checks the `tone_check_result` from the state. If it's "negative", the `story_generator` is called *again*, overwriting the `current_story` in the state. Otherwise, the flow ends.


=== "Java"

    The `runAsyncImpl` method orchestrates the sub-agents using RxJava's Flowable streams and operators for asynchronous control flow.


    **Explanation of Logic:**

    1. The initial `storyGenerator.runAsync(invocationContext)` Flowable is executed. Its output is expected to be in `invocationContext.session().state().get("current_story")`.
    2. The `loopAgent's` Flowable runs next (due to `Flowable.concatArray` and `Flowable.defer`). The LoopAgent internally calls the `critic` and `reviser` sub-agents sequentially for up to `maxIterations`. They read/write `current_story` and `criticism` from/to the state.
    3. Then, the `sequentialAgent's` Flowable executes. It calls the `grammar_check` then `tone_check`, reading `current_story` and writing `grammar_suggestions` and `tone_check_result` to the state.
    4. **Custom Part:** After the sequentialAgent completes, logic within a `Flowable.defer` checks the "tone_check_result" from `invocationContext.session().state()`. If it's "negative", the `storyGenerator` Flowable is *conditionally concatenated* and executed again, overwriting "current_story". Otherwise, an empty Flowable is used, and the overall workflow proceeds to completion.

---

### Part 3: Defining the LLM Sub-Agents

These are standard `LlmAgent` definitions, responsible for specific tasks. Their `output key` parameter is crucial for placing results into the `session.state` where other agents or the custom orchestrator can access them.

=== "Python"

    ```python
    GEMINI_2_FLASH = "gemini-2.5-flash" # Define model constant
    # --- Define the individual LLM agents ---
    story_generator = LlmAgent(
        name="StoryGenerator",
        model=GEMINI_2_FLASH,
        instruction="""You are a story writer. Write a short story (around 100 words) about a cat,
    based on the topic provided in session state with key 'topic'""",
        input_schema=None,
        output_key="current_story",  # Key for storing output in session state
    )
    critic = LlmAgent(
        name="Critic",
        model=GEMINI_2_FLASH,
        instruction="""You are a story critic. Review the story provided in
    session state with key 'current_story'. Provide 1-2 sentences of constructive criticism
    on how to improve it. Focus on plot or character.""",
        input_schema=None,
        output_key="criticism",  # Key for storing criticism in session state
    )
    reviser = LlmAgent(
        name="Reviser",
        model=GEMINI_2_FLASH,
        instruction="""You are a story reviser. Revise the story provided in
    session state with key 'current_story', based on the criticism in
    session state with key 'criticism'. Output only the revised story.""",
        input_schema=None,
        output_key="current_story",  # Overwrites the original story
    )
    grammar_check = LlmAgent(
        name="GrammarCheck",
        model=GEMINI_2_FLASH,
        instruction="""You are a grammar checker. Check the grammar of the story
    provided in session state with key 'current_story'. Output only the suggested
    corrections as a list, or output 'Grammar is good!' if there are no errors.""",
        input_schema=None,
        output_key="grammar_suggestions",
    )
    tone_check = LlmAgent(
        name="ToneCheck",
        model=GEMINI_2_FLASH,
        instruction="""You are a tone analyzer. Analyze the tone of the story
    provided in session state with key 'current_story'. Output only one word: 'positive' if
    the tone is generally positive, 'negative' if the tone is generally negative, or 'neutral'
    otherwise.""",
        input_schema=None,
        output_key="tone_check_result", # This agent's output determines the conditional flow
    )
    ```
=== "Java"



---

### Part 4: Instantiating and Running the custom agent

Finally, you instantiate your `StoryFlowAgent` and use the `Runner` as usual.

=== "Python"

    ```python
    # --- Create the custom agent instance ---
    story_flow_agent = StoryFlowAgent(
        name="StoryFlowAgent",
        story_generator=story_generator,
        critic=critic,
        reviser=reviser,
        grammar_check=grammar_check,
        tone_check=tone_check,
    )
    INITIAL_STATE = {"topic": "a brave kitten exploring a haunted house"}
    # --- Setup Runner and Session ---
    async def setup_session_and_runner():
        session_service = InMemorySessionService()
        session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID, state=INITIAL_STATE)
        logger.info(f"Initial session state: {session.state}")
        runner = Runner(
            agent=story_flow_agent, # Pass the custom orchestrator agent
            app_name=APP_NAME,
            session_service=session_service
        )
        return session_service, runner
    # --- Function to Interact with the Agent ---
    async def call_agent_async(user_input_topic: str):
        """
        Sends a new topic to the agent (overwriting the initial one if needed)
        and runs the workflow.
        """
        session_service, runner = await setup_session_and_runner()
        current_session = await session_service.get_session(app_name=APP_NAME,
                                                      user_id=USER_ID,
                                                      session_id=SESSION_ID)
        if not current_session:
            logger.error("Session not found!")
            return
        current_session.state["topic"] = user_input_topic
        logger.info(f"Updated session state topic to: {user_input_topic}")
        content = types.Content(role='user', parts=[types.Part(text=f"Generate a story about: {user_input_topic}")])
        events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)
        final_response = "No final response captured."
        async for event in events:
            if event.is_final_response() and event.content and event.content.parts:
                logger.info(f"Potential final response from [{event.author}]: {event.content.parts[0].text}")
                final_response = event.content.parts[0].text
        print("\n--- Agent Interaction Result ---")
        print("Agent Final Response: ", final_response)
        final_session = await session_service.get_session(app_name=APP_NAME,
                                                    user_id=USER_ID,
                                                    session_id=SESSION_ID)
        print("Final Session State:")
        import json
        print(json.dumps(final_session.state, indent=2))
        print("-------------------------------\n")
    # --- Run the Agent ---
    # Note: In Colab, you can directly use 'await' at the top level.
    # If running this code as a standalone Python script, you'll need to use asyncio.run() or manage the event loop.
    await call_agent_async("a lonely robot finding a friend in a junkyard")
    ```

=== "Java"



*(Note: The full runnable code, including imports and execution logic, can be found linked below.)*

---

## Full Code Example

???+ "Storyflow Agent"

    === "Python"

        ```python
        # Full runnable code for the StoryFlowAgent example
        # Copyright 2025 Google LLC
        #
        # Licensed under the Apache License, Version 2.0 (the "License");
        # you may not use this file except in compliance with the License.
        # You may obtain a copy of the License at
        #
        #     http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.

        import logging
        from typing import AsyncGenerator
        from typing_extensions import override

        from google.adk.agents import LlmAgent, BaseAgent, LoopAgent, SequentialAgent
        from google.adk.agents.invocation_context import InvocationContext
        from google.genai import types
        from google.adk.sessions import InMemorySessionService
        from google.adk.runners import Runner
        from google.adk.events import Event
        from pydantic import BaseModel, Field

        # --- Constants ---
        APP_NAME = "story_app"
        USER_ID = "12345"
        SESSION_ID = "123344"
        GEMINI_2_FLASH = "gemini-2.5-flash"

        # --- Configure Logging ---
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)


        # --- Custom Orchestrator Agent ---
        # --8<-- [start:init]
        class StoryFlowAgent(BaseAgent):
            """
            Custom agent for a story generation and refinement workflow.

            This agent orchestrates a sequence of LLM agents to generate a story,
            critique it, revise it, check grammar and tone, and potentially
            regenerate the story if the tone is negative.
            """

            # --- Field Declarations for Pydantic ---
            # Declare the agents passed during initialization as class attributes with type hints
            story_generator: LlmAgent
            critic: LlmAgent
            reviser: LlmAgent
            grammar_check: LlmAgent
            tone_check: LlmAgent

            loop_agent: LoopAgent
            sequential_agent: SequentialAgent

            # model_config allows setting Pydantic configurations if needed, e.g., arbitrary_types_allowed
            model_config = {"arbitrary_types_allowed": True}

            def __init__(
                self,
                name: str,
                story_generator: LlmAgent,
                critic: LlmAgent,
                reviser: LlmAgent,
                grammar_check: LlmAgent,
                tone_check: LlmAgent,
            ):
                """
                Initializes the StoryFlowAgent.

                Args:
                    name: The name of the agent.
                    story_generator: An LlmAgent to generate the initial story.
                    critic: An LlmAgent to critique the story.
                    reviser: An LlmAgent to revise the story based on criticism.
                    grammar_check: An LlmAgent to check the grammar.
                    tone_check: An LlmAgent to analyze the tone.
                """
                # Create internal agents *before* calling super().__init__
                loop_agent = LoopAgent(
                    name="CriticReviserLoop", sub_agents=[critic, reviser], max_iterations=2
                )
                sequential_agent = SequentialAgent(
                    name="PostProcessing", sub_agents=[grammar_check, tone_check]
                )

                # Define the sub_agents list for the framework
                sub_agents_list = [
                    story_generator,
                    loop_agent,
                    sequential_agent,
                ]

                # Pydantic will validate and assign them based on the class annotations.
                super().__init__(
                    name=name,
                    story_generator=story_generator,
                    critic=critic,
                    reviser=reviser,
                    grammar_check=grammar_check,
                    tone_check=tone_check,
                    loop_agent=loop_agent,
                    sequential_agent=sequential_agent,
                    sub_agents=sub_agents_list, # Pass the sub_agents list directly
                )
        # --8<-- [end:init]

            # --8<-- [start:executionlogic]
            @override
            async def _run_async_impl(
                self, ctx: InvocationContext
            ) -> AsyncGenerator[Event, None]:
                """
                Implements the custom orchestration logic for the story workflow.
                Uses the instance attributes assigned by Pydantic (e.g., self.story_generator).
                """
                logger.info(f"[{self.name}] Starting story generation workflow.")

                # 1. Initial Story Generation
                logger.info(f"[{self.name}] Running StoryGenerator...")
                async for event in self.story_generator.run_async(ctx):
                    logger.info(f"[{self.name}] Event from StoryGenerator: {event.model_dump_json(indent=2, exclude_none=True)}")
                    yield event

                # Check if story was generated before proceeding
                if "current_story" not in ctx.session.state or not ctx.session.state["current_story"]:
                     logger.error(f"[{self.name}] Failed to generate initial story. Aborting workflow.")
                     return # Stop processing if initial story failed

                logger.info(f"[{self.name}] Story state after generator: {ctx.session.state.get('current_story')}")


                # 2. Critic-Reviser Loop
                logger.info(f"[{self.name}] Running CriticReviserLoop...")
                # Use the loop_agent instance attribute assigned during init
                async for event in self.loop_agent.run_async(ctx):
                    logger.info(f"[{self.name}] Event from CriticReviserLoop: {event.model_dump_json(indent=2, exclude_none=True)}")
                    yield event

                logger.info(f"[{self.name}] Story state after loop: {ctx.session.state.get('current_story')}")

                # 3. Sequential Post-Processing (Grammar and Tone Check)
                logger.info(f"[{self.name}] Running PostProcessing...")
                # Use the sequential_agent instance attribute assigned during init
                async for event in self.sequential_agent.run_async(ctx):
                    logger.info(f"[{self.name}] Event from PostProcessing: {event.model_dump_json(indent=2, exclude_none=True)}")
                    yield event

                # 4. Tone-Based Conditional Logic
                tone_check_result = ctx.session.state.get("tone_check_result")
                logger.info(f"[{self.name}] Tone check result: {tone_check_result}")

                if tone_check_result == "negative":
                    logger.info(f"[{self.name}] Tone is negative. Regenerating story...")
                    async for event in self.story_generator.run_async(ctx):
                        logger.info(f"[{self.name}] Event from StoryGenerator (Regen): {event.model_dump_json(indent=2, exclude_none=True)}")
                        yield event
                else:
                    logger.info(f"[{self.name}] Tone is not negative. Keeping current story.")
                    pass

                logger.info(f"[{self.name}] Workflow finished.")
            # --8<-- [end:executionlogic]

        # --8<-- [start:llmagents]
        # --- Define the individual LLM agents ---
        story_generator = LlmAgent(
            name="StoryGenerator",
            model=GEMINI_2_FLASH,
            instruction="""You are a story writer. Write a short story (around 100 words) about a cat,
        based on the topic provided in session state with key 'topic'""",
            input_schema=None,
            output_key="current_story",  # Key for storing output in session state
        )

        critic = LlmAgent(
            name="Critic",
            model=GEMINI_2_FLASH,
            instruction="""You are a story critic. Review the story provided in
        session state with key 'current_story'. Provide 1-2 sentences of constructive criticism
        on how to improve it. Focus on plot or character.""",
            input_schema=None,
            output_key="criticism",  # Key for storing criticism in session state
        )

        reviser = LlmAgent(
            name="Reviser",
            model=GEMINI_2_FLASH,
            instruction="""You are a story reviser. Revise the story provided in
        session state with key 'current_story', based on the criticism in
        session state with key 'criticism'. Output only the revised story.""",
            input_schema=None,
            output_key="current_story",  # Overwrites the original story
        )

        grammar_check = LlmAgent(
            name="GrammarCheck",
            model=GEMINI_2_FLASH,
            instruction="""You are a grammar checker. Check the grammar of the story
        provided in session state with key 'current_story'. Output only the suggested
        corrections as a list, or output 'Grammar is good!' if there are no errors.""",
            input_schema=None,
            output_key="grammar_suggestions",
        )

        tone_check = LlmAgent(
            name="ToneCheck",
            model=GEMINI_2_FLASH,
            instruction="""You are a tone analyzer. Analyze the tone of the story
        provided in session state with key 'current_story'. Output only one word: 'positive' if
        the tone is generally positive, 'negative' if the tone is generally negative, or 'neutral'
        otherwise.""",
            input_schema=None,
            output_key="tone_check_result", # This agent's output determines the conditional flow
        )
        # --8<-- [end:llmagents]

        # --8<-- [start:story_flow_agent]
        # --- Create the custom agent instance ---
        story_flow_agent = StoryFlowAgent(
            name="StoryFlowAgent",
            story_generator=story_generator,
            critic=critic,
            reviser=reviser,
            grammar_check=grammar_check,
            tone_check=tone_check,
        )

        INITIAL_STATE = {"topic": "a brave kitten exploring a haunted house"}

        # --- Setup Runner and Session ---
        async def setup_session_and_runner():
            session_service = InMemorySessionService()
            session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID, state=INITIAL_STATE)
            logger.info(f"Initial session state: {session.state}")
            runner = Runner(
                agent=story_flow_agent, # Pass the custom orchestrator agent
                app_name=APP_NAME,
                session_service=session_service
            )
            return session_service, runner

        # --- Function to Interact with the Agent ---
        async def call_agent_async(user_input_topic: str):
            """
            Sends a new topic to the agent (overwriting the initial one if needed)
            and runs the workflow.
            """

            session_service, runner = await setup_session_and_runner()

            current_session = await session_service.get_session(app_name=APP_NAME,
                                                          user_id=USER_ID,
                                                          session_id=SESSION_ID)
            if not current_session:
                logger.error("Session not found!")
                return

            current_session.state["topic"] = user_input_topic
            logger.info(f"Updated session state topic to: {user_input_topic}")

            content = types.Content(role='user', parts=[types.Part(text=f"Generate a story about: {user_input_topic}")])
            events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)

            final_response = "No final response captured."
            async for event in events:
                if event.is_final_response() and event.content and event.content.parts:
                    logger.info(f"Potential final response from [{event.author}]: {event.content.parts[0].text}")
                    final_response = event.content.parts[0].text

            print("\n--- Agent Interaction Result ---")
            print("Agent Final Response: ", final_response)

            final_session = await session_service.get_session(app_name=APP_NAME,
                                                        user_id=USER_ID,
                                                        session_id=SESSION_ID)
            print("Final Session State:")
            import json
            print(json.dumps(final_session.state, indent=2))
            print("-------------------------------\n")

        # --- Run the Agent ---
        # Note: In Colab, you can directly use 'await' at the top level.
        # If running this code as a standalone Python script, you'll need to use asyncio.run() or manage the event loop.
        await call_agent_async("a lonely robot finding a friend in a junkyard")
        # --8<-- [end:story_flow_agent]
        ```

    === "Java"




# Agents

In the Agent Development Kit (ADK), an **Agent** is a self-contained execution unit designed to act autonomously to achieve specific goals. Agents can perform tasks, interact with users, utilize external tools, and coordinate with other agents.

The foundation for all agents in ADK is the `BaseAgent` class. It serves as the fundamental blueprint. To create functional agents, you typically extend `BaseAgent` in one of three main ways, catering to different needs ‚Äì from intelligent reasoning to structured process control.

<img src="../assets/agent-types.png" alt="Types of agents in ADK">

## Core Agent Categories

ADK provides distinct agent categories to build sophisticated applications:

1. [**LLM Agents (`LlmAgent`, `Agent`)**](llm-agents.md): These agents utilize Large Language Models (LLMs) as their core engine to understand natural language, reason, plan, generate responses, and dynamically decide how to proceed or which tools to use, making them ideal for flexible, language-centric tasks. [Learn more about LLM Agents...](llm-agents.md)

2. [**Workflow Agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`)**](workflow-agents/index.md): These specialized agents control the execution flow of other agents in predefined, deterministic patterns (sequence, parallel, or loop) without using an LLM for the flow control itself, perfect for structured processes needing predictable execution. [Explore Workflow Agents...](workflow-agents/index.md)

3. [**Custom Agents**](custom-agents.md): Created by extending `BaseAgent` directly, these agents allow you to implement unique operational logic, specific control flows, or specialized integrations not covered by the standard types, catering to highly tailored application requirements. [Discover how to build Custom Agents...](custom-agents.md)

## Choosing the Right Agent Type

The following table provides a high-level comparison to help distinguish between the agent types. As you explore each type in more detail in the subsequent sections, these distinctions will become clearer.

| Feature              | LLM Agent (`LlmAgent`)              | Workflow Agent                              | Custom Agent (`BaseAgent` subclass)      |
| :------------------- | :---------------------------------- | :------------------------------------------ |:-----------------------------------------|
| **Primary Function** | Reasoning, Generation, Tool Use     | Controlling Agent Execution Flow            | Implementing Unique Logic/Integrations   |
| **Core Engine**  | Large Language Model (LLM)          | Predefined Logic (Sequence, Parallel, Loop) | Custom Code                              |
| **Determinism**  | Non-deterministic (Flexible)        | Deterministic (Predictable)                 | Can be either, based on implementation   |
| **Primary Use**  | Language tasks, Dynamic decisions   | Structured processes, Orchestration         | Tailored requirements, Specific workflows|

## Agents Working Together: Multi-Agent Systems

While each agent type serves a distinct purpose, the true power often comes from combining them. Complex applications frequently employ [multi-agent architectures](multi-agents.md) where:

* **LLM Agents** handle intelligent, language-based task execution.
* **Workflow Agents** manage the overall process flow using standard patterns.
* **Custom Agents** provide specialized capabilities or rules needed for unique integrations.

Understanding these core types is the first step toward building sophisticated, capable AI applications with ADK.

---

## What's Next?

Now that you have an overview of the different agent types available in ADK, dive deeper into how they work and how to use them effectively:

* [**LLM Agents:**](llm-agents.md) Explore how to configure agents powered by large language models, including setting instructions, providing tools, and enabling advanced features like planning and code execution.
* [**Workflow Agents:**](workflow-agents/index.md) Learn how to orchestrate tasks using `SequentialAgent`, `ParallelAgent`, and `LoopAgent` for structured and predictable processes.
* [**Custom Agents:**](custom-agents.md) Discover the principles of extending `BaseAgent` to build agents with unique logic and integrations tailored to your specific needs.
* [**Multi-Agents:**](multi-agents.md) Understand how to combine different agent types to create sophisticated, collaborative systems capable of tackling complex problems.
* [**Models:**](models.md) Learn about the different LLM integrations available and how to select the right model for your agents.


# LLM Agent

The `LlmAgent` (often aliased simply as `Agent`) is a core component in ADK,
acting as the "thinking" part of your application. It leverages the power of a
Large Language Model (LLM) for reasoning, understanding natural language, making
decisions, generating responses, and interacting with tools.

Unlike deterministic [Workflow Agents](workflow-agents/index.md) that follow
predefined execution paths, `LlmAgent` behavior is non-deterministic. It uses
the LLM to interpret instructions and context, deciding dynamically how to
proceed, which tools to use (if any), or whether to transfer control to another
agent.

Building an effective `LlmAgent` involves defining its identity, clearly guiding
its behavior through instructions, and equipping it with the necessary tools and
capabilities.

## Defining the Agent's Identity and Purpose

First, you need to establish what the agent *is* and what it's *for*.

* **`name` (Required):** Every agent needs a unique string identifier. This
  `name` is crucial for internal operations, especially in multi-agent systems
  where agents need to refer to or delegate tasks to each other. Choose a
  descriptive name that reflects the agent's function (e.g.,
  `customer_support_router`, `billing_inquiry_agent`). Avoid reserved names like
  `user`.

* **`description` (Optional, Recommended for Multi-Agent):** Provide a concise
  summary of the agent's capabilities. This description is primarily used by
  *other* LLM agents to determine if they should route a task to this agent.
  Make it specific enough to differentiate it from peers (e.g., "Handles
  inquiries about current billing statements," not just "Billing agent").

* **`model` (Required):** Specify the underlying LLM that will power this
  agent's reasoning. This is a string identifier like `"gemini-2.5-flash"`. The
  choice of model impacts the agent's capabilities, cost, and performance. See
  the [Models](models.md) page for available options and considerations.

=== "Python"

    ```python
    # Example: Defining the basic identity
    capital_agent = LlmAgent(
        model="gemini-2.5-flash",
        name="capital_agent",
        description="Answers user questions about the capital city of a given country."
        # instruction and tools will be added next
    )
    ```

=== "Java"




## Guiding the Agent: Instructions (`instruction`)

The `instruction` parameter is arguably the most critical for shaping an
`LlmAgent`'s behavior. It's a string (or a function returning a string) that
tells the agent:

* Its core task or goal.
* Its personality or persona (e.g., "You are a helpful assistant," "You are a witty pirate").
* Constraints on its behavior (e.g., "Only answer questions about X," "Never reveal Y").
* How and when to use its `tools`. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.
* The desired format for its output (e.g., "Respond in JSON," "Provide a bulleted list").

**Tips for Effective Instructions:**

* **Be Clear and Specific:** Avoid ambiguity. Clearly state the desired actions and outcomes.
* **Use Markdown:** Improve readability for complex instructions using headings, lists, etc.
* **Provide Examples (Few-Shot):** For complex tasks or specific output formats, include examples directly in the instruction.
* **Guide Tool Use:** Don't just list tools; explain *when* and *why* the agent should use them.

**State:**

* The instruction is a string template, you can use the `{var}` syntax to insert dynamic values into the instruction.
* `{var}` is used to insert the value of the state variable named var.
* `{artifact.var}` is used to insert the text content of the artifact named var.
* If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a `?` to the variable name as in `{var?}`.

=== "Python"

    ```python
    # Example: Adding instructions
    capital_agent = LlmAgent(
        model="gemini-2.5-flash",
        name="capital_agent",
        description="Answers user questions about the capital city of a given country.",
        instruction="""You are an agent that provides the capital city of a country.
    When a user asks for the capital of a country:
    1. Identify the country name from the user's query.
    2. Use the `get_capital_city` tool to find the capital.
    3. Respond clearly to the user, stating the capital city.
    Example Query: "What's the capital of {country}?"
    Example Response: "The capital of France is Paris."
    """,
        # tools will be added next
    )
    ```

=== "Java"



*(Note: For instructions that apply to *all* agents in a system, consider using
`global_instruction` on the root agent, detailed further in the
[Multi-Agents](multi-agents.md) section.)*

## Equipping the Agent: Tools (`tools`)

Tools give your `LlmAgent` capabilities beyond the LLM's built-in knowledge or
reasoning. They allow the agent to interact with the outside world, perform
calculations, fetch real-time data, or execute specific actions.

* **`tools` (Optional):** Provide a list of tools the agent can use. Each item in the list can be:
    * A native function or method (wrapped as a `FunctionTool`). Python ADK automatically wraps the native function into a `FuntionTool` whereas, you must explicitly wrap your Java methods using `FunctionTool.create(...)`
    * An instance of a class inheriting from `BaseTool`.
    * An instance of another agent (`AgentTool`, enabling agent-to-agent delegation - see [Multi-Agents](multi-agents.md)).

The LLM uses the function/tool names, descriptions (from docstrings or the
`description` field), and parameter schemas to decide which tool to call based
on the conversation and its instructions.

=== "Python"

    ```python
    # Define a tool function
    def get_capital_city(country: str) -> str:
      """Retrieves the capital city for a given country."""
      # Replace with actual logic (e.g., API call, database lookup)
      capitals = {"france": "Paris", "japan": "Tokyo", "canada": "Ottawa"}
      return capitals.get(country.lower(), f"Sorry, I don't know the capital of {country}.")

    # Add the tool to the agent
    capital_agent = LlmAgent(
        model="gemini-2.5-flash",
        name="capital_agent",
        description="Answers user questions about the capital city of a given country.",
        instruction="""You are an agent that provides the capital city of a country... (previous instruction text)""",
        tools=[get_capital_city] # Provide the function directly
    )
    ```

=== "Java"



Learn more about Tools in the [Tools](../tools/index.md) section.

## Advanced Configuration & Control

Beyond the core parameters, `LlmAgent` offers several options for finer control:

### Fine-Tuning LLM Generation (`generate_content_config`)

You can adjust how the underlying LLM generates responses using `generate_content_config`.

* **`generate_content_config` (Optional):** Pass an instance of `google.genai.types.GenerateContentConfig` to control parameters like `temperature` (randomness), `max_output_tokens` (response length), `top_p`, `top_k`, and safety settings.

=== "Python"

    ```python
    from google.genai import types

    agent = LlmAgent(
        # ... other params
        generate_content_config=types.GenerateContentConfig(
            temperature=0.2, # More deterministic output
            max_output_tokens=250
        )
    )
    ```

=== "Java"



### Structuring Data (`input_schema`, `output_schema`, `output_key`)

For scenarios requiring structured data exchange with an `LLM Agent`, the ADK provides mechanisms to define expected input and desired output formats using schema definitions.

* **`input_schema` (Optional):** Define a schema representing the expected input structure. If set, the user message content passed to this agent *must* be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.

* **`output_schema` (Optional):** Define a schema representing the desired output structure. If set, the agent's final response *must* be a JSON string conforming to this schema.
    * **Constraint:** Using `output_schema` enables controlled generation within the LLM but **disables the agent's ability to use tools or transfer control to other agents**. Your instructions must guide the LLM to produce JSON matching the schema directly.

* **`output_key` (Optional):** Provide a string key. If set, the text content of the agent's *final* response will be automatically saved to the session's state dictionary under this key. This is useful for passing results between agents or steps in a workflow.
    * In Python, this might look like: `session.state[output_key] = agent_response_text`
    * In Java: `session.state().put(outputKey, agentResponseText)`

=== "Python"

    The input and output schema is typically a `Pydantic` BaseModel.

    ```python
    from pydantic import BaseModel, Field

    class CapitalOutput(BaseModel):
        capital: str = Field(description="The capital of the country.")

    structured_capital_agent = LlmAgent(
        # ... name, model, description
        instruction="""You are a Capital Information Agent. Given a country, respond ONLY with a JSON object containing the capital. Format: {"capital": "capital_name"}""",
        output_schema=CapitalOutput, # Enforce JSON output
        output_key="found_capital"  # Store result in state['found_capital']
        # Cannot use tools=[get_capital_city] effectively here
    )
    ```

=== "Java"

     The input and output schema is a `google.genai.types.Schema` object.



### Managing Context (`include_contents`)

Control whether the agent receives the prior conversation history.

* **`include_contents` (Optional, Default: `'default'`):** Determines if the `contents` (history) are sent to the LLM.
    * `'default'`: The agent receives the relevant conversation history.
    * `'none'`: The agent receives no prior `contents`. It operates based solely on its current instruction and any input provided in the *current* turn (useful for stateless tasks or enforcing specific contexts).

=== "Python"

    ```python
    stateless_agent = LlmAgent(
        # ... other params
        include_contents='none'
    )
    ```

=== "Java"



### Planning & Code Execution

![python_only](https://img.shields.io/badge/Currently_supported_in-Python-blue){ title="This feature is currently available for Python. Java support is planned/ coming soon."}

For more complex reasoning involving multiple steps or executing code:

* **`planner` (Optional):** Assign a `BasePlanner` instance to enable multi-step reasoning and planning before execution. (See [Multi-Agents](multi-agents.md) patterns).
* **`code_executor` (Optional):** Provide a `BaseCodeExecutor` instance to allow the agent to execute code blocks (e.g., Python) found in the LLM's response. ([See Tools/Built-in tools](../tools/built-in-tools.md)).

## Putting It Together: Example

??? "Code"
    Here's the complete basic `capital_agent`:

    === "Python"

        ```python
        # --- Full example code demonstrating LlmAgent with Tools vs. Output Schema ---
        import json # Needed for pretty printing dicts

        from google.adk.agents import LlmAgent
        from google.adk.runners import Runner
        from google.adk.sessions import InMemorySessionService
        from google.genai import types
        from pydantic import BaseModel, Field

        # --- 1. Define Constants ---
        APP_NAME = "agent_comparison_app"
        USER_ID = "test_user_456"
        SESSION_ID_TOOL_AGENT = "session_tool_agent_xyz"
        SESSION_ID_SCHEMA_AGENT = "session_schema_agent_xyz"
        MODEL_NAME = "gemini-2.5-flash"

        # --- 2. Define Schemas ---

        # Input schema used by both agents
        class CountryInput(BaseModel):
            country: str = Field(description="The country to get information about.")

        # Output schema ONLY for the second agent
        class CapitalInfoOutput(BaseModel):
            capital: str = Field(description="The capital city of the country.")
            # Note: Population is illustrative; the LLM will infer or estimate this
            # as it cannot use tools when output_schema is set.
            population_estimate: str = Field(description="An estimated population of the capital city.")

        # --- 3. Define the Tool (Only for the first agent) ---
        def get_capital_city(country: str) -> str:
            """Retrieves the capital city of a given country."""
            print(f"\n-- Tool Call: get_capital_city(country='{country}') --")
            country_capitals = {
                "united states": "Washington, D.C.",
                "canada": "Ottawa",
                "france": "Paris",
                "japan": "Tokyo",
            }
            result = country_capitals.get(country.lower(), f"Sorry, I couldn't find the capital for {country}.")
            print(f"-- Tool Result: '{result}' --")
            return result

        # --- 4. Configure Agents ---

        # Agent 1: Uses a tool and output_key
        capital_agent_with_tool = LlmAgent(
            model=MODEL_NAME,
            name="capital_agent_tool",
            description="Retrieves the capital city using a specific tool.",
            instruction="""You are a helpful agent that provides the capital city of a country using a tool.
        The user will provide the country name in a JSON format like {"country": "country_name"}.
        1. Extract the country name.
        2. Use the `get_capital_city` tool to find the capital.
        3. Respond clearly to the user, stating the capital city found by the tool.
        """,
            tools=[get_capital_city],
            input_schema=CountryInput,
            output_key="capital_tool_result", # Store final text response
        )

        # Agent 2: Uses output_schema (NO tools possible)
        structured_info_agent_schema = LlmAgent(
            model=MODEL_NAME,
            name="structured_info_agent_schema",
            description="Provides capital and estimated population in a specific JSON format.",
            instruction=f"""You are an agent that provides country information.
        The user will provide the country name in a JSON format like {{"country": "country_name"}}.
        Respond ONLY with a JSON object matching this exact schema:
        {json.dumps(CapitalInfoOutput.model_json_schema(), indent=2)}
        Use your knowledge to determine the capital and estimate the population. Do not use any tools.
        """,
            # *** NO tools parameter here - using output_schema prevents tool use ***
            input_schema=CountryInput,
            output_schema=CapitalInfoOutput, # Enforce JSON output structure
            output_key="structured_info_result", # Store final JSON response
        )

        # --- 5. Set up Session Management and Runners ---
        session_service = InMemorySessionService()

        # Create separate sessions for clarity, though not strictly necessary if context is managed
        session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID_TOOL_AGENT)
        session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID_SCHEMA_AGENT)

        # Create a runner for EACH agent
        capital_runner = Runner(
            agent=capital_agent_with_tool,
            app_name=APP_NAME,
            session_service=session_service
        )
        structured_runner = Runner(
            agent=structured_info_agent_schema,
            app_name=APP_NAME,
            session_service=session_service
        )

        # --- 6. Define Agent Interaction Logic ---
        async def call_agent_and_print(
            runner_instance: Runner,
            agent_instance: LlmAgent,
            session_id: str,
            query_json: str
        ):
            """Sends a query to the specified agent/runner and prints results."""
            print(f"\n>>> Calling Agent: '{agent_instance.name}' | Query: {query_json}")

            user_content = types.Content(role='user', parts=[types.Part(text=query_json)])

            final_response_content = "No final response received."
            async for event in runner_instance.run_async(user_id=USER_ID, session_id=session_id, new_message=user_content):
                # print(f"Event: {event.type}, Author: {event.author}") # Uncomment for detailed logging
                if event.is_final_response() and event.content and event.content.parts:
                    # For output_schema, the content is the JSON string itself
                    final_response_content = event.content.parts[0].text

            print(f"<<< Agent '{agent_instance.name}' Response: {final_response_content}")

            current_session = session_service.get_session(app_name=APP_NAME,
                                                          user_id=USER_ID,
                                                          session_id=session_id)
            stored_output = current_session.state.get(agent_instance.output_key)

            # Pretty print if the stored output looks like JSON (likely from output_schema)
            print(f"--- Session State ['{agent_instance.output_key}']: ", end="")
            try:
                # Attempt to parse and pretty print if it's JSON
                parsed_output = json.loads(stored_output)
                print(json.dumps(parsed_output, indent=2))
            except (json.JSONDecodeError, TypeError):
                 # Otherwise, print as string
                print(stored_output)
            print("-" * 30)


        # --- 7. Run Interactions ---
        async def main():
            print("--- Testing Agent with Tool ---")
            await call_agent_and_print(capital_runner, capital_agent_with_tool, SESSION_ID_TOOL_AGENT, '{"country": "France"}')
            await call_agent_and_print(capital_runner, capital_agent_with_tool, SESSION_ID_TOOL_AGENT, '{"country": "Canada"}')

            print("\n\n--- Testing Agent with Output Schema (No Tool Use) ---")
            await call_agent_and_print(structured_runner, structured_info_agent_schema, SESSION_ID_SCHEMA_AGENT, '{"country": "France"}')
            await call_agent_and_print(structured_runner, structured_info_agent_schema, SESSION_ID_SCHEMA_AGENT, '{"country": "Japan"}')

        if __name__ == "__main__":
            await main()

        ```
