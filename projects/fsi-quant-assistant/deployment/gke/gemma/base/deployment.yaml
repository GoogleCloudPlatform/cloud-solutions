# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Based on: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gemma-vllm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gemma-vllm
  template:
    metadata:
      labels:
        app: gemma-vllm
        ai.gke.io/model: gemma-3-1b-it
        ai.gke.io/inference-server: vllm
    spec:
      serviceAccount: gemma-agent-ksa
      containers:
        - name: vllm-server
          image: docker.io/vllm/vllm-openai:v0.10.0
          resources:
            requests:
              cpu: "2"
              memory: "10Gi"
              ephemeral-storage: "10Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "2"
              memory: "10Gi"
              ephemeral-storage: "10Gi"
              nvidia.com/gpu: "1"
          env:
            - name: VLLM_LOGGING_LEVEL
              value: DEBUG
            - name: LD_LIBRARY_PATH
              value: ${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64
            - name: MODEL_ID
              value: google/gemma-3-1b-it
          command: ["/bin/bash", "-c"]
          args:
            - |
              # 1. Use the official name for maximum compatibility
              # Set the Hugging Face token from the Kubernetes Secret
              export HUGGING_FACE_HUB_TOKEN=$(cat /mnt/secret-store/hugging_face_api_token.txt)

              # 2. Start the vLLM server
              python3 -m vllm.entrypoints.openai.api_server \
                --model $MODEL_ID \
                --port 8000 \
                --dtype bfloat16 \
                --tensor-parallel-size 1
          ports:
            - containerPort: 8000
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: secret-store
              mountPath: "/mnt/secret-store"
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: secret-store
          csi:
            driver: secrets-store-gke.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: secret-provider
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-l4
        cloud.google.com/gke-gpu-driver-version: latest
