{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Visual Inspection AI Model Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project id and location for the pipeline\n",
    "PROJECT_ID = 'visual-inspection-demo-2184'\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "# A staging bucket for the pipeline and training program\n",
    "STAGING_BUCKET='gs://viai-demo-data-us-central1/ml-trainings/viai-segmentation-deeplabv3'\n",
    "# The service account that the pipeline and training program acts as\n",
    "SERVICE_ACCOUNT = '1047381110578-compute@developer.gserviceaccount.com'\n",
    "\n",
    "# The dataset id in Vertex to use for model training\n",
    "DATASET_ID = '1850063553663336448'\n",
    "\n",
    "# Container image for the training program\n",
    "CONTAINER_LOCATION = 'us'\n",
    "CONTAINER_NAME = 'trainer'\n",
    "CONTAINER_TAG = 'latest'\n",
    "CONTAINER_URI = f'{CONTAINER_LOCATION}-docker.pkg.dev/{PROJECT_ID}/visual-inspection-ml-training/segmentation-deeplabv3plus-{CONTAINER_NAME}:{CONTAINER_TAG}'\n",
    "\n",
    "# Experiment to log metrics and parameters from the model training\n",
    "EXPERIMENT = f'{PROJECT_ID}-viai-segmentation-deeplabv3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q google-cloud-aiplatform[autologging]==1.65.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and initialize the Vertex AI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "JOB_NAME = f'{EXPERIMENT}-{TIMESTAMP}'\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, \n",
    "                location=LOCATION,\n",
    "                staging_bucket=STAGING_BUCKET)#, experiment=EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a custom job with the training container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai_custom_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=CONTAINER_URI,\n",
    "    model_serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-13:latest'\n",
    ")\n",
    "vertex_ai_custom_job.run(\n",
    "    machine_type='n1-standard-8',\n",
    "    replica_count=1,\n",
    "    accelerator_type = 'NVIDIA_TESLA_V100',\n",
    "    accelerator_count = 1,\n",
    "    dataset=aiplatform.ImageDataset(DATASET_ID, location=LOCATION),\n",
    "    annotation_schema_uri='gs://google-cloud-aiplatform/schema/dataset/annotation/image_segmentation_1.0.0.yaml',\n",
    "    args=[f'--experiment={EXPERIMENT}',\n",
    "          '--loss-function=dice_focal',\n",
    "          '--num-epochs=200',\n",
    "          '--batch-size=16',\n",
    "          '--patience-epochs=50'],\n",
    "    restart_job_on_worker_restart=True,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    sync=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
